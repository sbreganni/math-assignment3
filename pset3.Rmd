---
title: 'Math for Data Science: Problem Set 3'
author: "Sofia Breganni, David Moth, Adarsh Tripathi"
date: "2025-24-11"
output:
  html_document:
    df_print: paged
header-includes:
  - \usepackage{amsmath}
  - \usepackage{tcolorbox}
  - \newtcolorbox{answerbox}{colback=gray!15, colframe=black!50, arc=2mm, boxrule=0.5pt, left=3mm, right=3mm, top=3mm, bottom=3mm}
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

**Due Date:** Wednesday, December 3 by the end of the day (23:59).

**Instructions:** Please submit one solution set per group and include your group members' names at the top. Please write your solutions within this Rmd file, under the relevant question. Please submit the knitted output as a pdf. Make sure to show all code you used to arrive at the answer. However, please provide a brief, clear answer to every question rather than making us infer it from your output, and please avoid printing unnecessary output.

## 1. More Cat Compression

I have another cat named Lev. He was upset that Laszlo got to be in my course materials and he didn't. In this exercise we will explain to Lev why my initial choice of Laszlo was nothing personal.

![My other cat, Lev.](lev.jpg){#id .class width="200"}

a.  Load the image of Lev. Extract its red, green, and blue matrices and store them as separate objects called `r`, `g`, and `b`. Center the blue matrix and compute its variance-covariance matrix. What is the dimensionality of this variance-covariance matrix and why?

```{r, echo = TRUE, include = TRUE}
library(jpeg)
library(patchwork)
library(factoextra)

lev_the_cat <- readJPEG("lev.jpg")

r_lev <- lev_the_cat[,,1]
g_lev <- lev_the_cat[,,2]
b_lev <- lev_the_cat[,,3]

b_centered_lev <- scale(b_lev, center = TRUE, scale = FALSE)

vc_lev <- cov(b_centered_lev)

dim(vc_lev)
```

    ```{=latex}
    \begin{answerbox}
    The dimensionality of this covariance is m x m, since the covariance matrix pairs the covariance of each column against the     other, and since there are 200 columns (and 150 rows), then we end up with a 200 x 200 matrix.
    \end{answerbox}
    ```

b.  Find the total variance of the centered blue matrix for this image. Compare this to the total variance of the centered blue matrix for Laszlo.

```{r, echo = TRUE, include = TRUE}
total_variance_blue_lev <- sum(diag(vc_lev))

laszlo_the_cat <- readJPEG("laszlo.jpg")

r_laszlo <- laszlo_the_cat[,,1]
g_laszlo <- laszlo_the_cat[,,2]
b_laszlo <- laszlo_the_cat[,,3]

b_centered_laszlo <- scale(b_laszlo, center = TRUE, scale = FALSE)

vc_laszlo <- cov(b_centered_laszlo)

total_variance_blue_laszlo <- sum(diag(vc_laszlo))
```

    ```{=latex}
    \begin{answerbox}
    The total variance of the blue matrix for the picture of Lev is `r total_variance_blue_lev`, while for the picture of Laszlo     it is `r total_variance_blue_laszlo`.
    ```

c.  Use the `eigen` command to get the eigendecomposition of the blue variance-covariance matrix for Lev. Multiply the centered blue data matrix by the eigenvector corresponding to the largest eigenvalue to get the first principal component. Compute its variance.

```{r, echo = TRUE, include = TRUE}
eigs_lev <- eigen(vc_lev)

first_eigenvector_lev <- eigs_lev$vectors[,1]

PC1_lev <- b_centered_lev %*% first_eigenvector_lev

lev_var_PC1 <- var(PC1_lev)
lev_var_PC1
```

d.  Use the answers to the previous two questions to compute the proportion of variance explained for Lev's first principal component. Check your answer against a scree plot produced by the `fviz_eig` command.[^1] Compare to what we saw for Laszlo in the lab.

[^1]: If they are similar up to the second decimal place, that's good enough.

```{r, echo = TRUE, include = TRUE}
# Calculating Lev's

prop_explained_by_first_PC_lev <- lev_var_PC1 / total_variance_blue_lev

# Calculating Laszlo's 
eigs_laszlo <- eigen(vc_laszlo)

first_eigenvector_laszlo <- eigs_laszlo$vectors[,1]

PC1_laszlo <- b_centered_laszlo %*% first_eigenvector_laszlo

laszlo_var_PC1 <- var(PC1_laszlo)

prop_explained_by_first_PC_laszlo <- laszlo_var_PC1 / total_variance_blue_laszlo

# Creating the Scree plots
b_pca_lev <- prcomp(b_lev, center = TRUE, scale. = FALSE)
b_pca_laszlo <- prcomp(b_laszlo, center = TRUE, scale. = FALSE)

p1 <- fviz_eig(b_pca_lev, barfill = "blue", ncp = 5, addlabels = TRUE) +
  ggtitle("Lev Blue PCA") +
  ylim(0, 100) +   # fixed y-axis
  theme(plot.title = element_text(hjust = 0.5))

p2 <- fviz_eig(b_pca_laszlo, barfill = "blue", ncp = 5, addlabels = TRUE) +
  ggtitle("Laszlo Blue PCA") +
  ylim(0, 100) +   # same y-axis
  theme(plot.title = element_text(hjust = 0.5))

(p1 + p2) + plot_annotation(
  title = "Scree Plots of Blue Channel for Lev and Laszlo",
  theme = theme(plot.title = element_text(hjust = 0.5, size = 16))
)
```


    ```{=latex}
    \begin{answerbox}
    The proportion of the total variance captured by the first principal component of the blue matrix is for lev `r  prop_explained_by_first_PC_lev`, while for laszlo it is `r prop_explained_by_first_PC_laszlo`.
    ```


e.  Now compute Lev's second principal component. Compute the covariance of the first principal component with the second.[^2] Is this what you expected? Comment briefly.

[^2]: You can round to 10 decimal places.

```{r, echo = TRUE, include = TRUE}
second_eigenvector_lev <- eigs_lev$vectors[,2]

PC2_lev <- b_centered_lev %*% second_eigenvector_lev

first_second_covariance_lev <- cov(as.vector(PC1_lev), as.vector(PC2_lev))
first_second_covariance_lev <- round(first_second_covariance_lev, 10)
```

    ```{=latex}
    \begin{answerbox}
    The covariance of the two first principal components is 0. This makes sense, as principal components by design are orthogonal, so that the maximum amount of unique and "different" variance can be captured, as otherwise the data that is captured can be explained by the former principal component.
    ```

f.  Now let's run PCA on Lev's `r`, `g`, and `b` matrices using the `prcomp` function with the options `center=FALSE` and `scale.=FALSE`.[^3] Combine these objects into a list. Now, looping over a handful of numbers of principal components, reconstitute images of Lev as we did in your lab. How many principal components does it take to start to recognize Lev as a cat?

[^3]: Scaling and centering is desirable when your variables are on different scales, but in this case it messes up the colors.

```{r, echo = TRUE, include = TRUE}
r_pca <- prcomp(r_lev, center = FALSE, scale. = FALSE)
g_pca <- prcomp(g_lev, center = FALSE, scale. = FALSE)
b_pca <- prcomp(b_lev, center = FALSE, scale. = FALSE)
rgb_pca_lev <- list(r_pca, g_pca, b_pca)

vec <- c(1, 2, 5, 10, 12, 15, 20, 50, 100)

h <- nrow(r_lev)
w <- ncol(r_lev)

par(mfrow=c(3,3), mar=c(0.5,0.5,2,0.5)) 

for(i in vec){
  photo_i <- sapply(rgb_pca_lev, function(j) {
    new.RGB <- j$x[, 1:i] %*% t(j$rotation[, 1:i])
    (new.RGB - min(new.RGB)) / (max(new.RGB) - min(new.RGB))
  }, simplify = "array")
  
  plot(1, type="n", xlim=c(0, w), ylim=c(0, h), asp=1, axes=FALSE, xlab="", ylab="")
  
  rasterImage(photo_i, 0, 0, w, h)
  
  mtext(paste(i, "PCs"), side=3, line=0.5, cex=1.2)
}
```
    ```{=latex}
    \begin{answerbox}
    As can be seen above, it takes around 10 principal components to have a reasonable degree of confidence that we are indeed looking at a cat.
    ```

g.  Using what you learned from parts (b), (d), and (f) above, explain why Lev requires more principal components to be minimally represented than Laszlo, despite being equally beautiful.

    ```{=latex}
    \begin{answerbox}
    While indeed equally beautiful, Lev requires more principal components because the variance is more equally spread out across the different principal components (though, of course, the first is still much higher than the remaining). Relatively, choosing just a few principal components in the case of Lev yields only a smaller proportion of the total pixel variance. While Laszlo has higher total variance in the case of the blue matrix, it is more relatively more concentrated in the first principal component. As such, Laszlo requires fewer principal components to capture his beauty.
    \end{answerbox}
    ```


\begin{answerbox}
Lev requires more principal components to be minimally represented than Laszlo because Lev's image has more complex variation in color and texture across the pixels, as seen in the higher total variance of the blue matrix and in the distribution of variance across the principal components. Each additional principal component captures some of this extra variation, so more PCs are needed to start recognizing Lev as a cat.
\end{answerbox}

## 2. Penalized Regression

\newcommand{\m}[1]{\mathbf{#1}}

We will derive the estimator for ridge regression, which is one of several *penalized regression* methods.[^4] The process we will follow is very similar to the standard regression estimator we derived in your lab, but with a small change to the loss function. Rather than minimizing the sum of squared errors, we will minimize the sum of squared errors subject to a constraint:

[^4]: See p. 237-244 of ISL.

$$  ||\beta||_2^2 \leq s $$

where $s$ is just some constant chosen by the analyst. Recall that $||\beta||_2^2$ is the squared $L_2$ norm of the $\beta$ vector.[^5] This question will build on the concepts and data in the lab, so please revisit those materials if anything here is unclear.

[^5]: Also known as the Euclidian norm.

a.  Write down the Lagrangian for this constrained minimization problem. Please use matrix notation (including for the $L_2$ norm).

```{=latex}
\begin{answerbox}
    We want to minimize the sum of squared residuals:
    \begin{itemize}
        \item $\min\limits_{\beta} \;||y - \mathbf{X}\beta||_2^2$
        \item Where $||y - \mathbf{X}\beta||_2^2 = \sum_{i=1}^n(y_i - (\mathbf{X}\beta)_i)^2 = (\mathbf{y} - \mathbf{X}\boldsymbol{\beta})^T(\mathbf{y} - \mathbf{X}\boldsymbol{\beta})$
    \end{itemize}

    With an $L_2$ norm constraint:
    \begin{itemize}
        \item The $L_2^2$ norm will be: $||\beta||_2^2 = \sum_{i=1}^p\beta^2 = \boldsymbol{\beta^T\beta}$
        \item And the constraint function: $g(\beta) = \boldsymbol{\beta^T\beta}-s$
    \end{itemize}

    \[
        \mathcal{L}(\boldsymbol{\beta}, \lambda) = (\mathbf{y} - \mathbf{X}\boldsymbol{\beta})^T(\mathbf{y} - \mathbf{X}\boldsymbol{\beta}) + \lambda (\boldsymbol{\beta^T\beta}-s)
    \]
\end{answerbox}
```

b.  Take the first derivative of the Lagrangian with respect to $\beta$ and set it equal to $\mathbf{0}$ to get the first order condition. (If you're feeling retro, you can use the \href{https://www.math.uwaterloo.ca/~hwolkowi/matrixcookbook.pdf}{Matrix Cookbook} to help you.) Solve for $\hat{\beta}$.

```{=latex}
\begin{answerbox}
Solving the derivative step-by-step:
    \begin{itemize}
        \item Derivative of $(\mathbf{y} - \mathbf{X}\boldsymbol{\beta})^T(\mathbf{y} - \mathbf{X}\boldsymbol{\beta}) = \mathbf{y}^T \mathbf{y} - 2\mathbf{y}^TX\boldsymbol{\beta} + \boldsymbol{\beta}^T X^T X \boldsymbol{\beta}$
        \[
        \frac{d}{d\beta}(\mathbf{y}^T \mathbf{y} - 2\mathbf{y}^TX\boldsymbol{\beta} + \boldsymbol{\beta}^T X^T X \boldsymbol{\beta}) = - 2X^T\mathbf{y} + 2 X^T X \boldsymbol{\beta} 
        \]
        \item Derivative of $\lambda (\boldsymbol{\beta^T\beta}-s)$
        \[
        \frac{d}{d\beta}\lambda (\boldsymbol{\beta^T\beta}-s) = \lambda 2 \boldsymbol{\beta}
        \]
        \item Therefore:
        \[
        \frac{d\mathcal{L}(\boldsymbol{\beta}, \lambda)}{d\beta} = - 2X^T\mathbf{y} + 2 X^T X \boldsymbol{\beta} + \lambda 2 \boldsymbol{\beta}
        \]
    \end{itemize}
    
    
Setting the derivative = 0 and solving for $\hat{\beta}$:
    \begin{align*}
        - 2X^T\mathbf{y} + 2 X^T X \boldsymbol{\beta} + \lambda 2 \boldsymbol{\beta} 
        &= \mathbf{0}\\
        2 X^T X \boldsymbol{\beta} + \lambda 2 \boldsymbol{\beta} 
        &= 2X^T\mathbf{y}\\
        (X^T X + \lambda I) \boldsymbol{\beta}
        &= X^T\mathbf{y}\\
        \hat{\boldsymbol{\beta}}
        &= (X^T X + \lambda I)^{-1}(X^T\mathbf{y})
    \end{align*}
    
\end{answerbox}
```

c.  Compare your answer to the standard linear regression estimator. Under what condition are they the same?

d.  Using the same `BostonHousing` dataset and the same set of variables from the lab, compute the ridge regression $\hat{\beta}$ with the equation you found in part (b) above. First standardize your $\mathbf{X}$ matrix using the `scale()` function in `R`. Use the `glmnet()` function in the `glmnet` package to check your answer.[^6]

[^6]: Note that `glmnet`'s `lambda` parameter corresponds to your $\frac{\lambda}{N}$, where $N$ is the number of rows of your data. Also, your estimates may differ slightly from `glmnet`'s, at around the second decimal place. That's alright; this is likely due to `glmnet`'s optimization algorithm. It's computationally expensive to invert large matrices so `glmnet` is probably taking some more efficient but (slightly) less precise approach.

```{r, echo = TRUE, include = TRUE}

```

e.  For the sequence of lambdas below, make a plot with `lambda_seq` on the $x$-axis (increasing from 0 to 100) and the estimated $\beta$ coefficients for per capita crime rate in the town (in blue), proximity to the Charles River (in red), and nitric oxides concentration (in green) on the $y$-axis. Use `geom_line` to plot the coefficients and add a black horizontal line at $y=0$. Label your axes and include a legend.

```{r, echo = FALSE, include = TRUE}
lambda_seq <- c(10^seq(2, -1, by = -.1), 0)

```

f.  Discussion:

-   Based on your plot above, explain why ridge is one of a number of so-called "shrinkage" estimators.

    ```{=latex}
    \begin{answerbox}
    Your boxed content goes here.
    You can put text, Markdown, equations, etc.
    \end{answerbox}
    ```

-   Tie this back to the constrained optimization problem you solved to obtain the ridge regression $\hat{\beta}$. Can you see how a larger value of $\lambda$ (or equivalently a small value of $s$) corresponds to greater shrinkage?

    ```{=latex}
    \begin{answerbox}
    Your boxed content goes here.
    You can put text, Markdown, equations, etc.
    \end{answerbox}
    ```

-   You will learn more about this class of estimators and their virtues next semester, but do you have any intuitions as to when and why they might be desirable?

    ```{=latex}
    \begin{answerbox}
    Your boxed content goes here.
    You can put text, Markdown, equations, etc.
    \end{answerbox}
    ```
